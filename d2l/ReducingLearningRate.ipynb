{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ReducingLearningRate.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM24Php2s5+NbU1ak5PQLRw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qtuter1997/100-Days-Of-ML-Code/blob/master/d2l/ReducingLearningRate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2l32UJqC1TE"
      },
      "source": [
        "**Learning rate schedule and Adaptive learning rate methods for Deep Learning**\n",
        "\n",
        "---\n",
        "The most of methods to optimal model when train deep neural network is use reduce learning rate.\n",
        "\n",
        "1. Learning Rate schedules\n",
        "2. Adaptive learning rate methods\n",
        "\n",
        "Reference tutorial at [here](https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1).\n",
        "\n",
        "In this tutorial, i train convolutional neural network on [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) using differing learning rate schedules and adaptive learing rate methods to compare their model performance.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjnvmY7zG-Td"
      },
      "source": [
        "1. Learning rate schedules\n",
        "\n",
        "Reducing the learning rate according to a pre-defined schedule.\n",
        "\n",
        "  Common learning rate schdules include **time-base decay, step decay** and **exponential decay**.\n",
        "  \n",
        "*   Constant learning rate\n",
        "*   Time-base dacay\n",
        "*   Step decay\n",
        "*   Exponential decay\n",
        "\n",
        "In the first half of this tutorial\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PIEapBJUCxMi",
        "outputId": "8ae3173b-7d5e-4e4c-c352-e5ba27fb80a9"
      },
      "source": [
        "# Add library\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import SGD, Adam, Adagrad, Adadelta, RMSprop\n",
        "from tensorflow.keras.models import model_from_json\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HH6KoaGJQpYB"
      },
      "source": [
        "Load CIFAR10 data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGoUvFFRQnR7",
        "outputId": "ae22094a-4aec-4b6e-9511-7311b17e0a81"
      },
      "source": [
        "batch_size = 64 # 2^x to match GPU\n",
        "num_classes = 10\n",
        "epochs = 100\n",
        "\n",
        "# input image demensions\n",
        "img_row, img_col = 32, 32\n",
        "\n",
        "# The data, shuffled and split between train and test sets\n",
        "(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1art_RGSs9E",
        "outputId": "2c1de397-f384-43de-88b7-d7368e2fe63e"
      },
      "source": [
        "# In example, we only use cat [==3] and dog [==5]\n",
        "train_picks = np.ravel(np.logical_or(Y_train == 3, Y_train == 5))\n",
        "test_picks = np.ravel(np.logical_or(Y_test == 3, Y_test == 5))\n",
        "\n",
        "Y_train = np.array(Y_train[train_picks] == 5, dtype= int)\n",
        "Y_test = np.array(Y_test[test_picks] == 5, dtype= int)\n",
        "\n",
        "X_train = X_train[train_picks]\n",
        "X_test = X_test[test_picks]\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train: [[[[125 125 116]\n",
            "   [110 101  91]\n",
            "   [102  90  83]\n",
            "   ...\n",
            "   [202 207 214]\n",
            "   [200 205 212]\n",
            "   [202 208 214]]\n",
            "\n",
            "  [[142 146 142]\n",
            "   [146 144 139]\n",
            "   [176 172 170]\n",
            "   ...\n",
            "   [195 201 205]\n",
            "   [198 205 209]\n",
            "   [204 211 215]]\n",
            "\n",
            "  [[180 185 183]\n",
            "   [143 146 146]\n",
            "   [156 157 157]\n",
            "   ...\n",
            "   [122 111 113]\n",
            "   [139 128 131]\n",
            "   [158 147 150]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[104  82  41]\n",
            "   [101  80  39]\n",
            "   [101  81  38]\n",
            "   ...\n",
            "   [126 103  67]\n",
            "   [126 103  69]\n",
            "   [125 101  68]]\n",
            "\n",
            "  [[104  81  40]\n",
            "   [105  84  41]\n",
            "   [109  88  43]\n",
            "   ...\n",
            "   [138 113  78]\n",
            "   [137 113  80]\n",
            "   [137 112  81]]\n",
            "\n",
            "  [[105  83  42]\n",
            "   [108  87  45]\n",
            "   [115  94  50]\n",
            "   ...\n",
            "   [143 117  82]\n",
            "   [143 116  84]\n",
            "   [144 116  86]]]\n",
            "\n",
            "\n",
            " [[[110 104  97]\n",
            "   [142 135 118]\n",
            "   [151 146 120]\n",
            "   ...\n",
            "   [ 39  39  39]\n",
            "   [ 40  40  40]\n",
            "   [ 38  38  38]]\n",
            "\n",
            "  [[109 103  95]\n",
            "   [141 133 115]\n",
            "   [152 147 119]\n",
            "   ...\n",
            "   [ 36  36  37]\n",
            "   [ 40  40  40]\n",
            "   [ 36  36  36]]\n",
            "\n",
            "  [[105  98  90]\n",
            "   [142 133 114]\n",
            "   [151 145 117]\n",
            "   ...\n",
            "   [ 39  39  41]\n",
            "   [ 44  44  45]\n",
            "   [ 39  39  40]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[ 62  59  60]\n",
            "   [ 39  38  49]\n",
            "   [ 60  64  81]\n",
            "   ...\n",
            "   [ 42  44  56]\n",
            "   [ 46  44  55]\n",
            "   [ 47  44  55]]\n",
            "\n",
            "  [[ 62  57  56]\n",
            "   [ 52  51  61]\n",
            "   [ 58  61  78]\n",
            "   ...\n",
            "   [ 43  46  58]\n",
            "   [ 47  45  58]\n",
            "   [ 49  46  60]]\n",
            "\n",
            "  [[ 55  51  50]\n",
            "   [ 60  60  71]\n",
            "   [ 56  60  78]\n",
            "   ...\n",
            "   [ 45  48  60]\n",
            "   [ 47  46  60]\n",
            "   [ 51  48  62]]]\n",
            "\n",
            "\n",
            " [[[252   8  42]\n",
            "   [249  15  42]\n",
            "   [250   8  39]\n",
            "   ...\n",
            "   [251   1  11]\n",
            "   [251   0  15]\n",
            "   [251   1  30]]\n",
            "\n",
            "  [[255   7  43]\n",
            "   [252  15  44]\n",
            "   [253  13  42]\n",
            "   ...\n",
            "   [255   1  11]\n",
            "   [255   0  18]\n",
            "   [254   4  33]]\n",
            "\n",
            "  [[253   6  42]\n",
            "   [250  16  42]\n",
            "   [250  24  43]\n",
            "   ...\n",
            "   [254   1  10]\n",
            "   [254   0  20]\n",
            "   [252   9  37]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[254  66  94]\n",
            "   [252  62  92]\n",
            "   [253  64  93]\n",
            "   ...\n",
            "   [252  70 101]\n",
            "   [253  69 103]\n",
            "   [252  70 104]]\n",
            "\n",
            "  [[250  49  81]\n",
            "   [252  53  82]\n",
            "   [255  59  86]\n",
            "   ...\n",
            "   [254  70 103]\n",
            "   [255  68 100]\n",
            "   [254  59  89]]\n",
            "\n",
            "  [[236  37  68]\n",
            "   [249  48  76]\n",
            "   [250  42  73]\n",
            "   ...\n",
            "   [250  78 113]\n",
            "   [250  74 109]\n",
            "   [251  58  88]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[ 41  41  41]\n",
            "   [ 39  39  37]\n",
            "   [ 39  39  36]\n",
            "   ...\n",
            "   [100  83  57]\n",
            "   [ 85  70  49]\n",
            "   [103  85  58]]\n",
            "\n",
            "  [[ 41  41  41]\n",
            "   [ 40  40  38]\n",
            "   [ 41  41  39]\n",
            "   ...\n",
            "   [101  83  57]\n",
            "   [ 86  69  50]\n",
            "   [104  85  59]]\n",
            "\n",
            "  [[ 41  41  41]\n",
            "   [ 40  40  38]\n",
            "   [ 40  40  38]\n",
            "   ...\n",
            "   [102  83  56]\n",
            "   [ 85  67  49]\n",
            "   [ 94  74  49]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[ 56  44  38]\n",
            "   [ 56  43  37]\n",
            "   [ 55  43  37]\n",
            "   ...\n",
            "   [ 85  57  43]\n",
            "   [ 86  58  45]\n",
            "   [ 86  58  46]]\n",
            "\n",
            "  [[ 58  44  38]\n",
            "   [ 57  43  37]\n",
            "   [ 57  43  37]\n",
            "   ...\n",
            "   [ 84  57  47]\n",
            "   [ 84  57  45]\n",
            "   [ 87  60  46]]\n",
            "\n",
            "  [[ 60  45  38]\n",
            "   [ 60  45  38]\n",
            "   [ 59  44  37]\n",
            "   ...\n",
            "   [ 80  56  44]\n",
            "   [ 82  56  44]\n",
            "   [ 85  59  45]]]\n",
            "\n",
            "\n",
            " [[[ 41  29  21]\n",
            "   [ 43  28  20]\n",
            "   [ 43  26  16]\n",
            "   ...\n",
            "   [ 62  42  28]\n",
            "   [ 61  39  20]\n",
            "   [ 58  35  16]]\n",
            "\n",
            "  [[ 43  29  21]\n",
            "   [ 45  28  20]\n",
            "   [ 46  28  17]\n",
            "   ...\n",
            "   [ 88  68  56]\n",
            "   [ 61  40  23]\n",
            "   [ 60  37  21]]\n",
            "\n",
            "  [[ 45  28  22]\n",
            "   [ 45  27  19]\n",
            "   [ 46  27  17]\n",
            "   ...\n",
            "   [131 113 102]\n",
            "   [ 69  50  37]\n",
            "   [ 58  37  24]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[165 155 145]\n",
            "   [160 147 138]\n",
            "   [148 132 124]\n",
            "   ...\n",
            "   [230 207 196]\n",
            "   [228 201 189]\n",
            "   [238 203 195]]\n",
            "\n",
            "  [[162 152 142]\n",
            "   [161 150 140]\n",
            "   [148 135 126]\n",
            "   ...\n",
            "   [237 212 203]\n",
            "   [244 222 214]\n",
            "   [252 225 220]]\n",
            "\n",
            "  [[157 147 137]\n",
            "   [159 149 139]\n",
            "   [150 140 130]\n",
            "   ...\n",
            "   [248 216 212]\n",
            "   [252 225 222]\n",
            "   [242 208 205]]]\n",
            "\n",
            "\n",
            " [[[161 166 159]\n",
            "   [172 177 170]\n",
            "   [176 181 174]\n",
            "   ...\n",
            "   [150 159 142]\n",
            "   [158 167 150]\n",
            "   [154 163 146]]\n",
            "\n",
            "  [[148 153 146]\n",
            "   [147 152 145]\n",
            "   [146 151 144]\n",
            "   ...\n",
            "   [140 149 132]\n",
            "   [143 152 135]\n",
            "   [143 152 135]]\n",
            "\n",
            "  [[167 172 165]\n",
            "   [176 181 174]\n",
            "   [170 175 168]\n",
            "   ...\n",
            "   [153 162 145]\n",
            "   [155 164 147]\n",
            "   [154 163 146]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[138  95  63]\n",
            "   [123  78  46]\n",
            "   [144 100  63]\n",
            "   ...\n",
            "   [118 101  81]\n",
            "   [114  99  80]\n",
            "   [106  93  75]]\n",
            "\n",
            "  [[147 103  69]\n",
            "   [155 110  73]\n",
            "   [133  89  53]\n",
            "   ...\n",
            "   [118 101  78]\n",
            "   [119 102  80]\n",
            "   [121 104  83]]\n",
            "\n",
            "  [[143  99  67]\n",
            "   [152 107  72]\n",
            "   [134  89  54]\n",
            "   ...\n",
            "   [120 102  79]\n",
            "   [121 102  78]\n",
            "   [122 102  78]]]], Y_train: [[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMxl4Ox3LlHN"
      },
      "source": [
        "2. Adaptive learning rate methods"
      ]
    }
  ]
}